{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Градиентный спуск\n",
    "\n",
    "Покончив с перцептроном, Зюк несколько торопливо предложил посмотреть, как в подобной ситуации себя проявит логистический нейрон, основанный на градиентном спуске. В ответ на наши недоумевающие взгляды Зюк смущённо объяснил, что очень боится, что ему когда-нибудь попадётся какое-то \"необычное\" яблоко или груша. Тогда данные могут перестать быть линейно разделимыми, и он рискует зависнуть, обучая несходящийся перцептрон. Для взрослого Нейрянина допустить такое - большой позор.\n",
    "Конечно, можно поставить ограничение на число шагов алгоритма... Но если вспомнить, как выглядит график количества ошибок, сразу станет понятно, что мы рискуем получить очень плохой классификатор, если оборвём процесс обучения раньше, чем нужно.\n",
    "Мы пропросили Зюка не подсказывать нам слишком много на этот раз. Всё-таки мы хотим сами научиться программировать нейросети. Зюк вежливо согласился, но настоял, чтобы мы посмотрели на его заготовку. Ключевые моменты он спрятал, предоставив нам возможность попрактиковаться.\n",
    "/// Изучите код и реализуйте пропущенные функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d as p3\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "from ipywidgets import interact, RadioButtons, IntSlider, FloatSlider, Dropdown, BoundedFloatText\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Определим разные полезные функции\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"сигмоидальная функция, работает и с числами, и с векторами (поэлементно)\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"производная сигмоидальной функции, работает и с числами, и с векторами (поэлементно)\"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс Neuron нас немного напугал... Такая махина. Зюк заверил нас, что он только кажется большим. На самом деле можно было всё это написать в три-четыре строчки, но вот понять их - это было бы сложнее. Он также извинился за многословность и неоптимальность реализации, объясняя её тем, что \"на таком удобнее учиться\". Пришлось поверить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, weights, activation_function=sigmoid, activation_function_derivative=sigmoid_prime):\n",
    "        \"\"\"\n",
    "        weights - вертикальный вектор весов нейрона формы (m, 1), weights[0][0] - смещение\n",
    "        activation_function - активационная функция нейрона, сигмоидальная функция по умолчанию\n",
    "        activation_function_derivative - производная активационной функции нейрона\n",
    "        \"\"\"\n",
    "\n",
    "        assert weights.shape[1] == 1, \"Incorrect weight shape\"\n",
    "\n",
    "        self.w = weights\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_function_derivative = activation_function_derivative\n",
    "\n",
    "    def forward_pass(self, single_input):\n",
    "        \"\"\"\n",
    "        активационная функция логистического нейрона\n",
    "        single_input - вектор входов формы (m, 1), \n",
    "        первый элемент вектора single_input - единица (если вы хотите учитывать смещение)\n",
    "        \"\"\"\n",
    "\n",
    "        result = 0\n",
    "        for i in range(self.w.size):\n",
    "            result += float(self.w[i] * single_input[i])\n",
    "        return self.activation_function(result)\n",
    "\n",
    "    def summatory(self, input_matrix):\n",
    "        \"\"\"\n",
    "        Вычисляет результат сумматорной функции для каждого примера из input_matrix. \n",
    "        input_matrix - матрица примеров размера (n, m), каждая строка - отдельный пример,\n",
    "        n - количество примеров, m - количество переменных.\n",
    "        Возвращает вектор значений сумматорной функции размера (n, 1).\n",
    "        \"\"\"\n",
    "        # Этот метод необходимо реализовать\n",
    "\n",
    "        return input_matrix.dot(self.w)\n",
    "\n",
    "    def activation(self, summatory_activation):\n",
    "        \"\"\"\n",
    "        Вычисляет для каждого примера результат активационной функции,\n",
    "        получив на вход вектор значений сумматорной функций\n",
    "        summatory_activation - вектор размера (n, 1), \n",
    "        где summatory_activation[i] - значение суммматорной функции для i-го примера.\n",
    "        Возвращает вектор размера (n, 1), содержащий в i-й строке \n",
    "        значение активационной функции для i-го примера.\n",
    "        \"\"\"\n",
    "        # Этот метод необходимо реализовать\n",
    "\n",
    "        return self.activation_function(summatory_activation)\n",
    "\n",
    "    def vectorized_forward_pass(self, input_matrix):\n",
    "        \"\"\"\n",
    "        Векторизованная активационная функция логистического нейрона.\n",
    "        input_matrix - матрица примеров размера (n, m), каждая строка - отдельный пример,\n",
    "        n - количество примеров, m - количество переменных.\n",
    "        Возвращает вертикальный вектор размера (n, 1) с выходными активациями нейрона\n",
    "        (элементы вектора - float)\n",
    "        \"\"\"\n",
    "        return self.activation(self.summatory(input_matrix))\n",
    "\n",
    "    def SGD(self, X, y, batch_size, learning_rate=0.1, eps=1e-6, max_steps=200):\n",
    "        \"\"\"\n",
    "        Внешний цикл алгоритма градиентного спуска.\n",
    "        X - матрица входных активаций (n, m)\n",
    "        y - вектор правильных ответов (n, 1)\n",
    "        \n",
    "        learning_rate - константа скорости обучения\n",
    "        batch_size - размер батча, на основании которого \n",
    "        рассчитывается градиент и совершается один шаг алгоритма\n",
    "        \n",
    "        eps - критерий остановки номер один: если разница между значением целевой функции \n",
    "        до и после обновления весов меньше eps - алгоритм останавливается. \n",
    "        Вторым вариантом была бы проверка размера градиента, а не изменение функции,\n",
    "        что будет работать лучше - неочевидно. В заданиях используйте первый подход.\n",
    "        \n",
    "        max_steps - критерий остановки номер два: если количество обновлений весов \n",
    "        достигло max_steps, то алгоритм останавливается\n",
    "        \n",
    "        Метод возвращает 1, если отработал первый критерий остановки (спуск сошёлся) \n",
    "        и 0, если второй (спуск не достиг минимума за отведённое время).\n",
    "        \"\"\"\n",
    "\n",
    "        # Этот метод необходимо реализовать\n",
    "\n",
    "        indexes = np.arange(len(X))\n",
    "        step = 0\n",
    "        while step < max_steps:\n",
    "            step += 1\n",
    "            \n",
    "            batch_indexes = np.random.choice(indexes, batch_size, replace=False)\n",
    "            X_batch = X[batch_indexes]\n",
    "            y_batch = y[batch_indexes]\n",
    "            \n",
    "            is_need_stop = self.update_mini_batch(X_batch, y_batch, learning_rate, eps)\n",
    "            if is_need_stop == 1:\n",
    "                return 1\n",
    "            \n",
    "        return 0\n",
    "\n",
    "    def update_mini_batch(self, X, y, learning_rate, eps):\n",
    "        \"\"\"\n",
    "        X - матрица размера (batch_size, m)\n",
    "        y - вектор правильных ответов размера (batch_size, 1)\n",
    "        learning_rate - константа скорости обучения\n",
    "        eps - критерий остановки номер один: если разница между значением целевой функции \n",
    "        до и после обновления весов меньше eps - алгоритм останавливается. \n",
    "        \n",
    "        Рассчитывает градиент (не забывайте использовать подготовленные заранее внешние функции) \n",
    "        и обновляет веса нейрона. Если ошибка изменилась меньше, чем на eps - возвращаем 1, \n",
    "        иначе возвращаем 0.\n",
    "        \"\"\"\n",
    "        # Этот метод необходимо реализовать\n",
    "\n",
    "        error_before = J_quadratic(self, X, y)\n",
    "        an_grad = compute_grad_analytically(self, X, y)\n",
    "        self.w -= an_grad * learning_rate\n",
    "        error_after = J_quadratic(self, X, y)\n",
    "\n",
    "        return int(abs(error_after - error_before) < eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зюк добавил, что на практике стохастический градиентный спуск обычно ограничивается не по количеству шагов алгоритма, а по количеству эпох обучения. Мы с ужасом переглянулись. Выходит, что Зюк просто не понимает, что земляне редко живут дольше ста лет... \"Зюк, - грустно сказали мы, - скорее всего, нам не удастся дождаться окончания работы алгоритма, люди столько не живут\". Зюк сперва удивился, потом рассмеялся и объяснил: эпохой обучения называют предъявление всех примеров по одному разу. Обычно батчи формируют так: входные данные перемешиваются, после этого разбиваются на кусочки по batch_size штук в каждом. После того как все примеры хотя бы раз побывали в батче, данные перемешиваются снова. Мы облегчённо вздохнули. Сейчас нам интересно понаблюдать за отдельными изменениями весов, поэтому мы ограничиваем алгоритм не эпохами, а количеством обновлений весов.\n",
    "Далее он показал нам код для расчета целевой функции и её градиента. Приятно было осознавать, что целевую функцию можно будет очень легко заменить, если нам захочется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_quadratic(neuron, X, y):\n",
    "    \"\"\"\n",
    "    Оценивает значение квадратичной целевой функции.\n",
    "    Всё как в лекции, никаких хитростей.\n",
    "\n",
    "    neuron - нейрон, у которого есть метод vectorized_forward_pass, предсказывающий значения на выборке X\n",
    "    X - матрица входных активаций (n, m)\n",
    "    y - вектор правильных ответов (n, 1)\n",
    "        \n",
    "    Возвращает значение J (число)\n",
    "    \"\"\"\n",
    "    \n",
    "    assert y.shape[1] == 1, 'Incorrect y shape'\n",
    "    \n",
    "    return 0.5 * np.mean((neuron.vectorized_forward_pass(X) - y) ** 2)\n",
    "\n",
    "def J_quadratic_derivative(y, y_hat):\n",
    "    \"\"\"\n",
    "    Вычисляет вектор частных производных целевой функции по каждому из предсказаний.\n",
    "    y_hat - вертикальный вектор предсказаний,\n",
    "    y - вертикальный вектор правильных ответов,\n",
    "    \n",
    "    В данном случае функция смехотворно простая, но если мы захотим поэкспериментировать \n",
    "    с целевыми функциями - полезно вынести эти вычисления в отдельный этап.\n",
    "    \n",
    "    Возвращает вектор значений производной целевой функции для каждого примера отдельно.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert y_hat.shape == y.shape and y_hat.shape[1] == 1, 'Incorrect shapes'\n",
    "    \n",
    "    return (y_hat - y) / len(y)\n",
    "    \n",
    "def compute_grad_analytically(neuron, X, y, J_prime=J_quadratic_derivative):\n",
    "    \"\"\"\n",
    "    Аналитическая производная целевой функции\n",
    "    neuron - объект класса Neuron\n",
    "    X - вертикальная матрица входов формы (n, m), на которой считается сумма квадратов отклонений\n",
    "    y - правильные ответы для примеров из матрицы X\n",
    "    J_prime - функция, считающая производные целевой функции по ответам\n",
    "    \n",
    "    Возвращает вектор размера (m, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Вычисляем активации\n",
    "    # z - вектор результатов сумматорной функции нейрона на разных примерах\n",
    "    \n",
    "    z = neuron.summatory(X)\n",
    "    y_hat = neuron.activation(z)\n",
    "\n",
    "    # Вычисляем нужные нам частные производные\n",
    "    dy_dyhat = J_prime(y, y_hat)\n",
    "    dyhat_dz = neuron.activation_function_derivative(z)\n",
    "    \n",
    "    # осознайте эту строчку:\n",
    "    dz_dw = X\n",
    "\n",
    "    # а главное, эту:\n",
    "    grad = ((dy_dyhat * dyhat_dz).T).dot(dz_dw)\n",
    "    \n",
    "    # можно было написать в два этапа. Осознайте, почему получается одно и то же\n",
    "    # grad_matrix = dy_dyhat * dyhat_dz * dz_dw\n",
    "    # grad = np.sum(, axis=0)\n",
    "    \n",
    "    # Сделаем из горизонтального вектора вертикальный\n",
    "    grad = grad.T\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако нам в голову пришла тревожная мысль.. А вдруг при расчёте градиента допущена ошибка?\n",
    "Как мы говорили в лекциях, неплохо бы проверить, правильно ли мы считаем градиенты, до того, как куда-то по этим градиентам спускаться. Зюк, например, долго пытался понять, как нам удалось сделать тот градиентный спуск, что мы показывали вам в лекции под саундтрек из \"Секретных материалов\". А мы всего-то забыли одну матрицу обратить, когда производную считали.\n",
    "Сейчас мы проверим, правильно ли считаются производные целевой функции: мы реализуем подсчёт частных производных по определению, как $$\\frac{\\partial f}{\\partial x_i} = \\frac{f\\left(x_1,\\ldots,x_{i-1}, x_i + \\Delta x, x_{i+1}, \\ldots, x_d\\right) - f\\left(x_1,\\ldots, x_d\\right)}{\\Delta x}.$$ Это не определение, в определении был бы $\\lim _{\\Delta x\\to 0}$! Но если мы возьмём достаточно малое $\\Delta x$, то приближение будет неплохим.\n",
    "Иными словами, мы посчитаем целевую функцию, чуть-чуть поменяем какой-нибудь вес, после этого посчитаем целевую функцию еще раз, дальше применяем определение, то есть разделим разницу в целевой функции на изменение веса.\n",
    "После этого можно будет сравнить результаты, полученные с помощью аналитического и численного метода: они не должны сильно отличаться.\n",
    "Вы уже должны догадываться, почему мы не можем всегда обходиться только лишь численным нахождением производной, но пример, непосредственно это иллюстрирующий, появится на следующей неделе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_numerically(neuron, X, y, J=J_quadratic, eps=10e-2):\n",
    "    \"\"\"\n",
    "    Численная производная целевой функции\n",
    "    neuron - объект класса Neuron\n",
    "    X - вертикальная матрица входов формы (n, m), на которой считается сумма квадратов отклонений\n",
    "    y - правильные ответы для тестовой выборки X\n",
    "    J - целевая функция, градиент которой мы хотим получить\n",
    "    eps - размер $\\delta w$ (малого изменения весов)\n",
    "    \"\"\"\n",
    "\n",
    "    initial_cost = J(neuron, X, y)\n",
    "    w_0 = neuron.w\n",
    "    num_grad = np.zeros(w_0.shape)\n",
    "    \n",
    "    for i in range(len(w_0)):\n",
    "        \n",
    "        old_wi = neuron.w[i].copy()\n",
    "        # Меняем вес\n",
    "        neuron.w[i] += eps\n",
    "        \n",
    "        # Считаем новое значение целевой функции и вычисляем приближенное значение градиента\n",
    "        num_grad[i] = (J(neuron, X, y) - initial_cost)/eps\n",
    "        \n",
    "        # Возвращаем вес обратно. Лучше так, чем -= eps, чтобы не накапливать ошибки округления\n",
    "        neuron.w[i] = old_wi\n",
    "            \n",
    "    # проверим, что не испортили нейрону веса своими манипуляциями\n",
    "    assert np.allclose(neuron.w, w_0), \"МЫ ИСПОРТИЛИ НЕЙРОНУ ВЕСА\"\n",
    "    return num_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, что у нас получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Численный градиент: \n [[ 0.01329644]\n [-0.00913862]\n [ 0.02702455]]\nАналитический градиент: \n [[ 0.01134039]\n [-0.0104192 ]\n [ 0.02717064]]\n"
     ]
    }
   ],
   "source": [
    "# Подготовим данные\n",
    "\n",
    "data = np.loadtxt(\"hw_1/data.csv\", delimiter=\",\")\n",
    "\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "X = np.hstack((np.ones((len(y), 1)), X))\n",
    "y = y.reshape((len(y), 1)) # Обратите внимание на эту очень противную и важную строчку\n",
    "\n",
    "\n",
    "# Создадим нейрон\n",
    "\n",
    "w = np.random.random((X.shape[1], 1))\n",
    "neuron = Neuron(w, activation_function=sigmoid, activation_function_derivative=sigmoid_prime)\n",
    "\n",
    "# Посчитаем пример\n",
    "num_grad = compute_grad_numerically(neuron, X, y, J=J_quadratic)\n",
    "an_grad = compute_grad_analytically(neuron, X, y, J_prime=J_quadratic_derivative)\n",
    "\n",
    "print(\"Численный градиент: \\n\", num_grad)\n",
    "print(\"Аналитический градиент: \\n\", an_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Вроде бы похоже\", но это не очень удовлетворительный ответ. Давайте посмотрим, как меняется наше приближение в зависимости от $\\varepsilon$. Посчитаем для разных $\\varepsilon$ модуль разности этих двух градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0217064465788\n"
     ]
    }
   ],
   "source": [
    "def print_grad_diff(eps):\n",
    "    num_grad = compute_grad_numerically(neuron, X, y, J=J_quadratic, eps=float(eps))\n",
    "    an_grad = compute_grad_analytically(neuron, X, y, J_prime=J_quadratic_derivative)\n",
    "    print(np.linalg.norm(num_grad-an_grad))\n",
    "    \n",
    "interact(print_grad_diff, \n",
    "            eps=RadioButtons(options=[\"3\", \"1\", \"0.1\", \"0.001\", \"0.0001\"]), separator=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неплохо, но можно лучше, причем с минимумом усилий. Давайте вместо того, чтобы считать $\\frac{f(x) - f(x + \\Delta x)}{\\Delta x}$, посмотрим на $\\frac{f(x  + \\Delta x) - f(x - \\Delta x)}{2 \\Delta x}$, то есть шагнём в обе стороны. Говорят, что на практике этот метод работает лучше.\n",
    "Реализуйте функцию compute_grad_numerically_2, чтобы проверить, слухи это или нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_numerically_2(neuron, X, y, J=J_quadratic, eps=10e-2):\n",
    "    \"\"\"\n",
    "    Численная производная целевой функции.\n",
    "    neuron - объект класса Neuron с вертикальным вектором весов w,\n",
    "    X - вертикальная матрица входов формы (n, m), на которой считается сумма квадратов отклонений,\n",
    "    y - правильные ответы для тестовой выборки X,\n",
    "    J - целевая функция, градиент которой мы хотим получить,\n",
    "    eps - размер $\\delta w$ (малого изменения весов).\n",
    "    \"\"\"\n",
    "    \n",
    "    # эту функцию необходимо реализовать\n",
    "    \n",
    "    w_0 = neuron.w\n",
    "    num_grad = np.zeros(w_0.shape)\n",
    "    \n",
    "    for i in range(len(w_0)):\n",
    "        \n",
    "        old_wi = neuron.w[i].copy()\n",
    "        # Меняем вес\n",
    "        neuron.w[i] += eps\n",
    "        cost_plus = J(neuron, X, y)\n",
    "        \n",
    "        neuron.w[i] = old_wi\n",
    "        \n",
    "        neuron.w[i] -= eps\n",
    "        cost_minus = J(neuron, X, y)\n",
    "        \n",
    "        # Считаем новое значение целевой функции и вычисляем приближенное значение градиента\n",
    "        num_grad[i] = (cost_plus - cost_minus)/(2*eps)\n",
    "        \n",
    "        # Возвращаем вес обратно. Лучше так, чем -= eps, чтобы не накапливать ошибки округления\n",
    "        neuron.w[i] = old_wi\n",
    "            \n",
    "    # проверим, что не испортили нейрону веса своими манипуляциями\n",
    "    assert np.allclose(neuron.w, w_0), \"МЫ ИСПОРТИЛИ НЕЙРОНУ ВЕСА\"\n",
    "    return num_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, к какому результату привели ваши эксперименты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0228516605858\n"
     ]
    }
   ],
   "source": [
    "def print_grad_diff_2(eps):\n",
    "    num_grad = compute_grad_numerically_2(neuron, X, y, J=J_quadratic, eps=float(eps))\n",
    "    an_grad = compute_grad_analytically(neuron, X, y, J_prime=J_quadratic_derivative)\n",
    "    print(np.linalg.norm(num_grad-an_grad))\n",
    "    \n",
    "interact(print_grad_diff_2, \n",
    "            eps=RadioButtons(options=[\"3\", \"1\", \"0.1\", \"0.001\", \"0.0001\"]), separator=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед нами тот редкий случай, когда фраза \"на порядок лучше\" - очевидное преуменьшение!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Мы будем проверять ваш алгоритм update_mini_batch на данных разного размера.\n",
    "# Пример данных, на которых вы можете проверить работу своего решения самостоятельно:\n",
    "np.random.seed(42)\n",
    "n = 10\n",
    "m = 5\n",
    "\n",
    "X = 20 * np.random.sample((n, m)) - 10\n",
    "y = (np.random.random(n) < 0.5).astype(np.int)[:, np.newaxis]\n",
    "w = 2 * np.random.random((m, 1)) - 1\n",
    "\n",
    "neuron = Neuron(w)\n",
    "neuron.update_mini_batch(X, y, 0.1, 1e-5)\n",
    "print(neuron.w)\n",
    "\n",
    "# Если вы посмотрите на веса нейрона neuron после выполнения этого кода, то они должны быть такими:\n",
    "# [[-0.22571548]\n",
    "# [-0.45367083]\n",
    "# [ 0.65670199]\n",
    "# [-0.27851325]\n",
    "# [-0.41341191]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация\n",
    "\n",
    "Мы сидели, уставшие, но довольные проделанной работой, как вдруг Зюк оживлённо воскликнул:\n",
    "-- У нас ведь всего два входа!\n",
    "-- И что? - удивлённо спросили мы.\n",
    "-- А то, что если мы зафиксируем значение смещения, то можем полноценно визуализировать целевую функцию. Неужели вам не хочется посмотреть, как она выглядит в этой задачке? А еще.. А еще давайте добавим возможность перемешивать и сдвигать данные! Как вам такая идея?\n",
    "Не дождавшись ответа, Зюк начал программировать. Он выглядел очень возбуждённым."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_by_weights(weights, X, y, bias):\n",
    "    \"\"\"\n",
    "    Посчитать значение целевой функции для нейрона с заданными весами.\n",
    "    Только для визуализации\n",
    "    \"\"\"\n",
    "    new_w = np.hstack((bias, weights)).reshape((3,1))\n",
    "    return J_quadratic(Neuron(new_w), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "max_b = 40\n",
    "min_b = -40\n",
    "max_w1 = 40\n",
    "min_w1 = -40\n",
    "max_w2 = 40\n",
    "min_w2 = -40\n",
    "\n",
    "g_bias = 0 # график номер 2 будет при первой генерации по умолчанию иметь то значение b, которое выставлено в первом\n",
    "X_corrupted = X.copy()\n",
    "y_corrupted = y.copy()\n",
    "\n",
    "@interact(fixed_bias=FloatSlider(min=min_b, max=max_b, continuous_update=False), \n",
    "          mixing=FloatSlider(min=0, max=1, continuous_update=False, value=0),\n",
    "          shifting=FloatSlider(min=0, max=1, continuous_update=False, value=0)\n",
    "            )\n",
    "def visualize_cost_function(fixed_bias, mixing, shifting):\n",
    "    \"\"\"\n",
    "    Визуализируем поверхность целевой функции на (опционально) подпорченных данных и сами данные.\n",
    "    Портим данные мы следующим образом: сдвигаем категории навстречу друг другу, на величину, равную shifting \n",
    "    Кроме того, меняем классы некоторых случайно выбранных примеров на противоположнее.\n",
    "    Доля таких примеров задаётся переменной mixing\n",
    "    \n",
    "    Нам нужно зафиксировать bias на определённом значении, чтобы мы могли что-нибудь визуализировать.\n",
    "    Можно посмотреть, как bias влияет на форму целевой функции\n",
    "    \"\"\"\n",
    "    xlim = (min_w1, max_w1)\n",
    "    ylim = (min_w2, max_w2)\n",
    "    xx = np.linspace(*xlim, num=101)\n",
    "    yy = np.linspace(*ylim, num=101)\n",
    "    xx, yy = np.meshgrid(xx, yy)\n",
    "    points = np.stack([xx, yy], axis=2)\n",
    "    \n",
    "    # не будем портить исходные данные, будем портить их копию\n",
    "    corrupted = data.copy()\n",
    "    \n",
    "    # инвертируем ответы для случайно выбранного поднабора данных\n",
    "    mixed_subset = np.random.choice(range(len(corrupted)), int(mixing * len(corrupted)), replace=False)\n",
    "    corrupted[mixed_subset, -1] = np.logical_not(corrupted[mixed_subset, -1])\n",
    "    \n",
    "    # сдвинем все груши (внизу справа) на shifting наверх и влево\n",
    "    pears = corrupted[:, 2] == 1\n",
    "    apples = np.logical_not(pears)\n",
    "    corrupted[pears, 0] -= shifting\n",
    "    corrupted[pears, 1] += shifting\n",
    "    \n",
    "    # вытащим наружу испорченные данные\n",
    "    global X_corrupted, y_corrupted\n",
    "    X_corrupted = np.hstack((np.ones((len(corrupted),1)), corrupted[:, :-1]))\n",
    "    y_corrupted = corrupted[:, -1].reshape((len(corrupted), 1))\n",
    "    \n",
    "    # посчитаем значения целевой функции на наших новых данных\n",
    "    calculate_weights = partial(J_by_weights, X=X_corrupted, y=y_corrupted, bias=fixed_bias)\n",
    "    J_values = np.apply_along_axis(calculate_weights, -1, points)\n",
    "    \n",
    "    fig = plt.figure(figsize=(16,5))\n",
    "    # сначала 3D-график целевой функции\n",
    "    ax_1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    surf = ax_1.plot_surface(xx, yy, J_values, alpha=0.3)\n",
    "    ax_1.set_xlabel(\"$w_1$\")\n",
    "    ax_1.set_ylabel(\"$w_2$\")\n",
    "    ax_1.set_zlabel(\"$J(w_1, w_2)$\")\n",
    "    ax_1.set_title(\"$J(w_1, w_2)$ for fixed bias = ${}$\".format(fixed_bias))\n",
    "    # потом плоский поточечный график повреждённых данных\n",
    "    ax_2 = fig.add_subplot(1, 2, 2)\n",
    "    plt.scatter(corrupted[apples][:, 0], corrupted[apples][:, 1], color = \"red\", alpha=0.7)\n",
    "    plt.scatter(corrupted[pears][:, 0], corrupted[pears][:, 1], color = \"green\", alpha=0.7)\n",
    "    ax_2.set_xlabel(\"yellowness\")\n",
    "    ax_2.set_ylabel(\"symmetry\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы с интересом рассматривали результат. По форме функции становилось понятно, что градиентный спуск будет обычно попадать в ловушку. На графике видна огромная яма, прижимающаяся своим дном к нулю.\n",
    "Это происходит от того, что данные линейно разделимы, и мы всегда можем увеличить хорошие веса, получив еще более хорошие (подумайте, почему это так).\n",
    "Еще страшнее выглядят плоские участки. Как вы думаете, застрянем мы не них или нет?\n",
    "Следующим графиком Зюк особенно нас порадовал. Можно было выбрать точку и bias, откуда мы хотим начать градиентный спуск, после чего его программа генерировала \"историю обучения\" (learning curve) - график значений целевой функции после очередного обновления весов.\n",
    "Смотря на поверхность целевой функции мы старались предсказать, к каким весам мы придём, если начнём спуск с той или иной точки. Оказалось что всё не так очевидно, как мы думали.\n",
    "На графике ниже видно, что при \"хорошем\" раскладе (мы попали в большую яму) значение целевой функции всё время уменьшалось (хоть и по чуть-чуть), пока мы не достигали порога по количеству итераций. При плохом мы могли просто застрять в локальном минимуме или выйти на плато.\n",
    "\\\\ Как можно бороться с подобными ловушками мы поговорим на третьей и четвёртой неделях.\n",
    "Если в предыдущем задании вы \"подпортили\" данные - график будет строиться именно для подпорченных =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEnCAYAAACKbmVGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d94a038cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(b=BoundedFloatText(value=str(g_bias), min=min_b, max=max_b, description=\"Enter $b$:\"),\n",
    "          w1=BoundedFloatText(value=\"0\", min=min_w1, max=max_w1, description=\"Enter $w_1$:\"),\n",
    "          w2=BoundedFloatText(value=\"0\", min=min_w2, max=max_w2, description=\"Enter $w_2$:\"),\n",
    "          learning_rate=Dropdown(options=[\"0.01\", \"0.05\", \"0.1\", \"0.5\", \"1\", \"5\", \"10\"], \n",
    "                                value=\"0.01\", description=\"Learning rate: \")\n",
    "         )\n",
    "def learning_curve_for_starting_point(b, w1, w2, learning_rate=0.1):\n",
    "    w = np.array([b, w1, w2]).reshape(X_corrupted.shape[1], 1)\n",
    "    learning_rate=float(learning_rate)\n",
    "    neuron = Neuron(w, activation_function=sigmoid, activation_function_derivative=sigmoid_prime)\n",
    "\n",
    "    story = [J_quadratic(neuron, X_corrupted, y_corrupted)]\n",
    "    for _ in range(2000):\n",
    "        neuron.SGD(X_corrupted, y_corrupted, 2, learning_rate=learning_rate, max_steps=2)\n",
    "        story.append(J_quadratic(neuron, X_corrupted, y_corrupted))\n",
    "    plt.plot(story)\n",
    "    \n",
    "    plt.title(\"Learning curve.\\n Final $b={0:.3f}$, $w_1={1:.3f}, w_2={2:.3f}$\".format(*neuron.w.ravel()))\n",
    "    plt.ylabel(\"$J(w_1, w_2)$\")\n",
    "    plt.xlabel(\"Weight and bias update number\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}